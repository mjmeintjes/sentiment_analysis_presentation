{
 "metadata": {
  "name": "Sentiment Analysis Example"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pprint import pprint"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 361
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Training data set"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos_tweets = [('I love this new Samsung cellphone - its easy to use and looks great', 'positive'),\n",
      "              ('This new cellphone is great', 'positive'),\n",
      "              (\"My new television is amazing, I'm very happy I bought it\", 'positive'),\n",
      "              (\"This is the best software - it's so easy to use\", 'positive'),\n",
      "              ('I am excited about Windows 8', 'positive'),\n",
      "              (\"I was really impressed by the excellect service I received\", 'positive'),\n",
      "]\n",
      "\n",
      "neg_tweets = [('I hate Windows 8', 'negative'),\n",
      "              ('My new cellphone is terrible', 'negative'),              \n",
      "              ('This is the worst software ever. It crashes every time I try to do something', 'negative'),\n",
      "              ('I hate my stupid new car, I am so frustrated', 'negative'),\n",
      "              (\"If this stupid computer crashes again I'm going to throw it out the window\", 'negative'),\n",
      "              (\"I believe 100% that this is the worst piece of software ever!\", 'negative'),\n",
      "]\n",
      "training_data = pos_tweets + neg_tweets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 362
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Tokenization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import word_tokenize, sent_tokenize\n",
      "def tokenize(tweet): \n",
      "    tweet = sent_tokenize(tweet)\n",
      "    return word_tokenize(\" \".join(tweet))\n",
      "\n",
      "print tokenize('Tokenize this please! This as well.')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['Tokenize', 'this', 'please', '!', 'This', 'as', 'well', '.']\n"
       ]
      }
     ],
     "prompt_number": 363
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Normalization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "swords = stopwords.words('english')\n",
      "#print \"Stopwords:\"\n",
      "#print \"----------\"\n",
      "#print swords\n",
      "\n",
      "def normalize(tweet):\n",
      "    \n",
      "    #convert all words to lower case    \n",
      "    tweet = [word.lower() for word in tweet]\n",
      "    \n",
      "    #remove stopwords    \n",
      "    tweet = [word for word in tweet if word not in swords]\n",
      "    \n",
      "    #remove short words\n",
      "    tweet = [word for word in tweet if len(word)>3]\n",
      "    \n",
      "    return tweet\n",
      "\n",
      "print normalize(['NorMalize', 'this', 'Please'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['normalize', 'please']\n"
       ]
      }
     ],
     "prompt_number": 364
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Test this pipeline"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_pipeline(inpt, pipeline, debug=False):\n",
      "    \"\"\"responsible for processing each input item through the pipeline\"\"\"\n",
      "    for tweet in inpt:\n",
      "        #we want this function to work with both classified and unclassified tweets\n",
      "        try:\n",
      "            tweet, cls = tweet\n",
      "        except:\n",
      "            cls = None\n",
      "        if debug: print \"Starting to process '{0}' through pipeline\".format(tweet)\n",
      "        for func in pipeline:            \n",
      "            tweet = func(tweet)\n",
      "            if debug: \n",
      "                print \"processed using pipeline function '{0}' and received output:\".format(func.__name__)\n",
      "                pprint(tweet)\n",
      "        if cls:\n",
      "            yield tweet, cls\n",
      "        else:\n",
      "            yield tweet\n",
      "        if debug: print\n",
      "\n",
      "pipeline = [tokenize, normalize]    \n",
      "processed = list(process_pipeline(training_data, pipeline))\n",
      "pprint(processed)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(['love', 'samsung', 'cellphone', 'easy', 'looks', 'great'], 'positive'),\n",
        " (['cellphone', 'great'], 'positive'),\n",
        " (['television', 'amazing', 'happy', 'bought'], 'positive'),\n",
        " (['best', 'software', 'easy'], 'positive'),\n",
        " (['excited', 'windows'], 'positive'),\n",
        " (['really', 'impressed', 'excellect', 'service', 'received'], 'positive'),\n",
        " (['hate', 'windows'], 'negative'),\n",
        " (['cellphone', 'terrible'], 'negative'),\n",
        " (['worst', 'software', 'ever.', 'crashes', 'every', 'time', 'something'],\n",
        "  'negative'),\n",
        " (['hate', 'stupid', 'frustrated'], 'negative'),\n",
        " (['stupid', 'computer', 'crashes', 'going', 'throw', 'window'], 'negative'),\n",
        " (['believe', 'worst', 'piece', 'software', 'ever'], 'negative')]\n"
       ]
      }
     ],
     "prompt_number": 365
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Feature extraction 1 - First make a list of all possible words in training data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def word_set(tweets):\n",
      "    \"\"\"responsible for extracting a set of all words used in all tweets in the training set\"\"\"\n",
      "\n",
      "    def get_all_words(tweets):    \n",
      "        \"\"\"iterates through all the tweets, and yields each word\"\"\"\n",
      "        for tweet, cls in tweets:\n",
      "            for word in tweet:\n",
      "                yield word\n",
      "\n",
      "    all_words = get_all_words(tweets)    \n",
      "    #deduplicate the words\n",
      "    all_words = set(all_words) \n",
      "    return sorted(all_words) #sort it to ensure consistency between runs\n",
      "\n",
      "\n",
      "text_cleaning_pipeline = [tokenize, normalize]\n",
      "word_set = word_set(process_pipeline(training_data, text_cleaning_pipeline))\n",
      "print \"Word set (all words in all tweets in training set)\"\n",
      "print \"--------------------------------------------------\"\n",
      "print word_set\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Word set (all words in all tweets in training set)\n",
        "--------------------------------------------------\n",
        "['amazing', 'believe', 'best', 'bought', 'cellphone', 'computer', 'crashes', 'easy', 'ever', 'ever.', 'every', 'excellect', 'excited', 'frustrated', 'going', 'great', 'happy', 'hate', 'impressed', 'looks', 'love', 'piece', 'really', 'received', 'samsung', 'service', 'software', 'something', 'stupid', 'television', 'terrible', 'throw', 'time', 'window', 'windows', 'worst']\n"
       ]
      }
     ],
     "prompt_number": 366
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Feature extraction 2 - Write function to map a tweet to the list of all possible words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def extract_features(tweet):\n",
      "    feature_set = {}\n",
      "    for word in word_set: #iterate through the whole set of words\n",
      "        feature_set['contains({0})'.format(word)] = word in tweet\n",
      "    return feature_set\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 367
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Putting it all together"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_tweet_index = 4\n",
      "\n",
      "final_pipeline = text_cleaning_pipeline + [extract_features]\n",
      "processed_data = list(process_pipeline([training_data[test_tweet_index]], final_pipeline, debug=True))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting to process 'I am excited about Windows 8' through pipeline\n",
        "processed using pipeline function 'tokenize' and received output:\n",
        "['I', 'am', 'excited', 'about', 'Windows', '8']\n",
        "processed using pipeline function 'normalize' and received output:\n",
        "['excited', 'windows']\n",
        "processed using pipeline function 'extract_features' and received output:\n",
        "{'contains(amazing)': False,\n",
        " 'contains(believe)': False,\n",
        " 'contains(best)': False,\n",
        " 'contains(bought)': False,\n",
        " 'contains(cellphone)': False,\n",
        " 'contains(computer)': False,\n",
        " 'contains(crashes)': False,\n",
        " 'contains(easy)': False,\n",
        " 'contains(ever)': False,\n",
        " 'contains(ever.)': False,\n",
        " 'contains(every)': False,\n",
        " 'contains(excellect)': False,\n",
        " 'contains(excited)': True,\n",
        " 'contains(frustrated)': False,\n",
        " 'contains(going)': False,\n",
        " 'contains(great)': False,\n",
        " 'contains(happy)': False,\n",
        " 'contains(hate)': False,\n",
        " 'contains(impressed)': False,\n",
        " 'contains(looks)': False,\n",
        " 'contains(love)': False,\n",
        " 'contains(piece)': False,\n",
        " 'contains(really)': False,\n",
        " 'contains(received)': False,\n",
        " 'contains(samsung)': False,\n",
        " 'contains(service)': False,\n",
        " 'contains(software)': False,\n",
        " 'contains(something)': False,\n",
        " 'contains(stupid)': False,\n",
        " 'contains(television)': False,\n",
        " 'contains(terrible)': False,\n",
        " 'contains(throw)': False,\n",
        " 'contains(time)': False,\n",
        " 'contains(window)': False,\n",
        " 'contains(windows)': True,\n",
        " 'contains(worst)': False}\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 368
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Train classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import NaiveBayesClassifier\n",
      "final_pipeline = [tokenize, normalize, extract_features]\n",
      "processed = list(process_pipeline(training_data, final_pipeline))\n",
      "\n",
      "classifier = NaiveBayesClassifier.train(processed)\n",
      "classifier.show_most_informative_features(35) \n",
      "#convert to prob (den/den+nom)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Most Informative Features\n",
        "     contains(cellphone) = True           positi : negati =      1.7 : 1.0\n",
        "      contains(software) = True           negati : positi =      1.7 : 1.0\n",
        "         contains(worst) = False          positi : negati =      1.4 : 1.0\n",
        "         contains(great) = False          negati : positi =      1.4 : 1.0\n",
        "          contains(hate) = False          positi : negati =      1.4 : 1.0\n",
        "        contains(stupid) = False          positi : negati =      1.4 : 1.0\n",
        "          contains(easy) = False          negati : positi =      1.4 : 1.0\n",
        "       contains(crashes) = False          positi : negati =      1.4 : 1.0\n",
        "      contains(software) = False          positi : negati =      1.2 : 1.0\n",
        "     contains(cellphone) = False          negati : positi =      1.2 : 1.0\n",
        "         contains(piece) = False          positi : negati =      1.2 : 1.0\n",
        "     contains(impressed) = False          negati : positi =      1.2 : 1.0\n",
        "       contains(believe) = False          positi : negati =      1.2 : 1.0\n",
        "         contains(every) = False          positi : negati =      1.2 : 1.0\n",
        "       contains(service) = False          negati : positi =      1.2 : 1.0\n",
        "    contains(television) = False          negati : positi =      1.2 : 1.0\n",
        "       contains(samsung) = False          negati : positi =      1.2 : 1.0\n",
        "      contains(terrible) = False          positi : negati =      1.2 : 1.0\n",
        "          contains(love) = False          negati : positi =      1.2 : 1.0\n",
        "         contains(throw) = False          positi : negati =      1.2 : 1.0\n",
        "     contains(something) = False          positi : negati =      1.2 : 1.0\n",
        "         contains(happy) = False          negati : positi =      1.2 : 1.0\n",
        "     contains(excellect) = False          negati : positi =      1.2 : 1.0\n",
        "        contains(really) = False          negati : positi =      1.2 : 1.0\n",
        "      contains(computer) = False          positi : negati =      1.2 : 1.0\n",
        "          contains(time) = False          positi : negati =      1.2 : 1.0\n",
        "          contains(best) = False          negati : positi =      1.2 : 1.0\n",
        "         contains(ever.) = False          positi : negati =      1.2 : 1.0\n",
        "          contains(ever) = False          positi : negati =      1.2 : 1.0\n",
        "         contains(going) = False          positi : negati =      1.2 : 1.0\n",
        "      contains(received) = False          negati : positi =      1.2 : 1.0\n",
        "         contains(looks) = False          negati : positi =      1.2 : 1.0\n",
        "        contains(window) = False          positi : negati =      1.2 : 1.0\n",
        "       contains(amazing) = False          negati : positi =      1.2 : 1.0\n",
        "       contains(excited) = False          negati : positi =      1.2 : 1.0\n"
       ]
      }
     ],
     "prompt_number": 369
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Test classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def classify_tweets(tweets):\n",
      "    processed_tweets = list(process_pipeline(tweets, final_pipeline))\n",
      "    for index, tweet in enumerate(processed_tweets):        \n",
      "        tweet_features = [w for w,v in tweet.items() if v]\n",
      "        prob = classifier.prob_classify(tweet)\n",
      "        predict = classifier.classify(tweet)\n",
      "        print \"I'm pretty sure ({0:.2f}%) that the tweet '{1}' is {2} because it {3}\".format(\n",
      "                prob.prob(predict) * 100, tweets[index], predict.upper(), \" and \".join(tweet_features))\n",
      "        print\n",
      "\n",
      "classify_tweets([\n",
      "                 \"I LOVE using #SuperBudget to do my monthly budget - so easy!\",\n",
      "                 \"Impressed with #SuperBudget - its so easy to use even my stupid brother can use it\",\n",
      "                 \"#SuperBudget is the worst, can't get anything done\",\n",
      "                 \"I love how #SuperBudget crashes each time I try to save my budget :(\",                 \n",
      "                 ])\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "I'm pretty sure (97.84%) that the tweet 'I LOVE using #SuperBudget to do my monthly budget - so easy!' is POSITIVE because it contains(love) and contains(easy)\n",
        "\n",
        "I'm pretty sure (86.22%) that the tweet 'Impressed with #SuperBudget - its so easy to use even my stupid brother can use it' is POSITIVE because it contains(impressed) and contains(stupid) and contains(easy)\n",
        "\n",
        "I'm pretty sure (80.36%) that the tweet '#SuperBudget is the worst, can't get anything done' is NEGATIVE because it contains(worst)\n",
        "\n",
        "I'm pretty sure (80.36%) that the tweet 'I love how #SuperBudget crashes each time I try to save my budget :(' is NEGATIVE because it contains(love) and contains(time) and contains(crashes)\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 370
    }
   ],
   "metadata": {}
  }
 ]
}